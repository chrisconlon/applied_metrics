\documentclass[aspectratio=169]{beamer}
\usetheme{metropolis}

%\usetheme{Darmstadt}
%\usepackage{times}
%\usefonttheme{structurebold}

\usepackage[english]{babel}
%\usepackage[table]{xcolor}
\usepackage{amsmath}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps}
\usepackage{amsmath,amssymb,setspace}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{relsize}

%\usepackage{pxfonts}

\usepackage[absolute,overlay]{textpos} 
\newenvironment{reference}[2]{% 
  \begin{textblock*}{\textwidth}(#1,#2) 
      \footnotesize\it\bgroup\color{red!50!black}}{\egroup\end{textblock*}} 

\DeclareMathSizes{10}{10}{6}{6} 


\title [Bootstrap]{Bootstrap and Subsampling}
\author{C.Conlon}
\institute{Applied Econometrics II}
\date{\today}
\setbeamerfont{equation}{size=\tiny}
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Bootstrap}
\begin{itemize}
\item Bootstrap takes a different approach.
\begin{itemize}
\item Instead of estimating $\hat{\theta}$ and then using a first-order Taylor Approximation...
\item What if we directly tried to construct the \alert{sampling distribution} of $\hat{\theta}$?
\end{itemize}
\item Our data $(X_1,\ldots,X_n) \sim P$ are drawn from some measure $P$
\begin{itemize}
\item We can form a \alert{nonparametric estimate} $\hat{P}$ by just assuming that each $X_i$ has weight $\frac{1}{n}$.
\item We can then simulate a new sample $X^{*} = (X_1^{*},\ldots X_n^{*}) \sim \hat{P}$.
\begin{itemize}
\item Easy: we take our data and construct $n$ observations by \alert{sampling with replacement} 
\end{itemize}
\item Compute whatever statistic of $X^{*}$, $S(X^*)$ we would like.
\begin{itemize}
\item Could be the OLS coefficients $\beta_1^{*},\ldots, \beta_k^{*}$.
\item Or some function $\beta_1^{*}/\beta_2^{*}$.
\item Or something really complicated: estimate parameters of a game $\hat{\theta}^*$ and now find Nash Equilibrium of the game $S(X^{*},\hat{\theta^*})$ changes.
\end{itemize}
\item Do this $B$ times and calculate at $Var(S_b)$ or $CI(S_1,\ldots, S_b)$.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Bootstrap: Bias Correction}
\small
The main idea is that $\hat{\theta}^{1*},\ldots, \hat{\theta}^{B*}$ approximates the \alert{sampling distribution} of $\hat{\theta}$. There are lots of things we can do now:
\begin{itemize}
\item We already saw how to calculate $Var(\hat{\theta}^{1*},\ldots, \hat{\theta}^{B*})$.
\begin{eqnarray*}
\frac{1}{B-1} \sum_{b=1}^B (\hat{\theta}_{(b)}^* - \overline{\theta^{*}})^2
\end{eqnarray*}
\item Calculate $E(\hat{\theta}^{*}_{(1)},\ldots, \hat{\theta}^{*}_{(B)}) = \overline{\theta^{*}} = \frac{1}{B} \sum_{b=1}^B \hat{\theta}_{(b)}^*$.
\end{itemize}
\end{frame}

\begin{frame}{Bootstrap: Bias Correction}
\begin{itemize}
\item We can use the estimated bias to \alert{bias correct} our estimates
\begin{eqnarray*}
Bias(\hat{\theta}) &=&E[\hat{\theta}] - \theta \\
Bias_{bs}(\hat{\theta}) &=&\overline{\theta^{*}} -\hat{\theta}
\end{eqnarray*}
Recall $\theta = E[\hat{\theta}] - Bias[\hat{\theta}]$:
\begin{eqnarray*}
\hat{\theta}- Bias_{bs}(\hat{\theta}) = \hat{\theta}-(\overline{\theta^{*}}-\hat{\theta}) = 2 \hat{\theta} - \overline{\theta^{*}}
\end{eqnarray*}
\item Correcting bias isn't for free - variance tradeoff!
\item Linear models are (hopefully) unbiased, but most nonlinear models are \alert{consistent but biased}.
\end{itemize}

\end{frame}

\begin{frame}{Bootstrap: Confidence Intervals}
There are actually three ways to construct bootstrap CI's:
\begin{enumerate}
\item Obvious way: sort  $\hat{\theta}^{*}$ then take $CI: [\hat{\theta}^{*}_{\alpha/2},\hat{\theta}^{*}_{1-\alpha/2}]$.
\item Asymptotic Normal:  $CI: \hat{\theta} \pm 1.96 \sqrt{V(\hat{\theta}^{*})}$. (CLT).
\item Better Way: let $W= \hat{\theta} -\theta$. If we knew the distribution of $W$ then: $Pr(w_{1-\alpha/2} \leq W \leq w_{\alpha/2})$:
\begin{eqnarray*}
CI: [\hat{\theta} -w_{1-\alpha/2}, \hat{\theta} -w_{\alpha/2}]
\end{eqnarray*}
We can estimate with $W^{*} = \hat{\theta}^{*} - \hat{\theta}$.
\begin{eqnarray*}
CI: [\hat{\theta} -w^*_{1-\alpha/2}, \hat{\theta} -w^*_{\alpha/2}] = [2 \hat{\theta} -\theta^*_{1-\alpha/2}, 2 \hat{\theta} -\theta^*_{\alpha/2}]
\end{eqnarray*}
Why is this preferred? Bias Correction!
\end{enumerate}

\end{frame}


\begin{frame}{Bootstrap: Why do people like it?}
\begin{itemize}
\item Econometricians like the bootstrap because under certain conditions it is \alert{higher order efficient} for the confidence interval construction (but not the standard errors).
\begin{itemize}
\item Intuition: because it is non-parametric it is able to deal with more than just the first term in the Taylor Expansion (actually an \alert{Edgeworth Expansion}).
\item Higher-order asymptotic theory is best left for real econometricians!
\end{itemize}
\item Practitioner's like the bootstrap because it is easy.
\begin{itemize}
\item If you can estimate your model once in a reasonable amount of time, then you can construct confidence intervals for most parameters and model predictions.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Bootstrap: When Does It Fail?}
\begin{itemize}
\item Bootstrap isn't magic. If you are constructing standard errors for something that isn't asymptotically normal, don't expect it to work!
\item The Bootstrap exploits the notion that your sample is IID (by sampling with replacement). If IID does not hold, the bootstrap may fail (but we can sometimes fix it!).
\item Bootstrap depends on asymptotic theory. In small samples weird things can happen. We need $\hat{P}$ to be a good approximation to the true $P$ (nothing missing).
\end{itemize}
\end{frame}

\begin{frame}{Bootstrap: Variants}
The bootstrap I have presented is sometimes known as the \alert{nonparametric bootstrap} and is the most common one.
\begin{description}
\item[Parametric Bootstrap] ex: if $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ then we can estimate $(\hat{\beta}_0,\hat{\beta}_1)$ via OLS.\\
 Now we can generate a bootstrap sample by drawing an $x_i$ at random with replacement $\hat{\beta}_0 + \hat{\beta}_1$ and then drawing \alert{independently} from the distribution of estimated residuals $\hat{\epsilon}_i$.
 \item[Wild Bootstrap] Similar to parametric bootstrap but we rescale $\epsilon_i$ to allow for \alert{heteroskedasticity}
\item[Block Bootstrap] For correlated data (e.g.: time series). Blocks can be overlapping or not.  
\end{description}
\end{frame}

\begin{frame}{Bootstrap vs Delta Method}
\begin{itemize}
\item Delta Method works best when working out Jacobian $D(\theta)$ is easy and statistic is well approximated with a linear function (not too curvy).
\item I would almost always advise Bootstrap unless:
\begin{itemize}
\item Delta method is trivial e.g.: $\beta_1 / \beta_2$ in linear regression.
\item Computing model takes many days so that 10,000 repetitions would be impossible.
\end{itemize}
\item Worst case scenario: rent time on Amazon EC2!
\begin{itemize}
\item I ``bought'' over \$1,000 of standard errors recently.
\end{itemize}
\item But neither is magic and both can fail!
\end{itemize}
\end{frame}


\begin{frame}{Subsampling}
The bootstrap has a close cousin \alert{subsampling}.
\begin{itemize}
\item  In practice it looks similar, but the underlying theory is quite different.
\item It relies on weaker assumptions and works even in some cases where the bootstrap fails.
\item Again ``fails'' means that the 95\% confidence interval has coverage that isn't very close to 95\%.
\end{itemize}
\end{frame}


\begin{frame}{Subsampling: How does it work?}
\begin{enumerate}
\item Draw a \alert{smaller} sample $X^*=(X_1^*,\ldots,X_{a_n}^*)$ \alert{without replacement} of size $a_n$ where as $n \rightarrow \infty$ we have $a_n \rightarrow \infty$ and $\frac{a_n}{n} \rightarrow 0$.
\begin{itemize}
\item e.g. $a_n = \log n$ or $a_n = \sqrt{n}$. Note that $a_n / 10$ doesn't work.
\end{itemize}
\item Compute the relevant statistic $\theta(X^*)$ or $g(\theta(X^*))$.
\item Repeat this $B$ times and construct the CDF:
\begin{align*}
L_n(t) = \frac{1}{B} \sum_{b=1}^B \mathbf{I} \left( \sqrt{a_n}(\widehat{\theta}_b - \widehat{\theta}_n  )  \leq t \right)
\end{align*}
\item Calculate the quantiles of the CDF and CI:
\begin{align*}
\hat{t}_{\alpha / 2}&=L_{n}^{-1}(\alpha / 2), \quad \hat{t}_{1-\alpha / 2}=L_{n}^{-1}(1-\alpha / 2)\\
C_{n}&=\left[\hat{\theta}_{n}-\frac{\hat{t}_{1-\alpha / 2}}{\sqrt{n}}, \hat{\theta}_{n}-\frac{\hat{t}_{\alpha / 2}}{\sqrt{n}}\right]
\end{align*}
\end{enumerate}
\end{frame}



\begin{frame}{Subsampling: Caveats}
\begin{itemize}
\item The proof for \alert{why} subsampling works is complicated. See \url{https://web.stanford.edu/~doubleh/lecturenotes/lecture13.pdf}.
\item Downsides:
\begin{itemize}
\item Subsampling really leans on $n \rightarrow \infty$ more than bootstrap. People often use bootstrap to understand finite sample performance (is this a good idea though?).
\item Choice of $a_n$ is difficult. Calculating the optimal value can be quite complicated and there aren't great rules of thumb.
\end{itemize}
\item But if you're in a weird case where bootstrap fails (parameter on the boundary, etc.) try subsampling and see!
\end{itemize}
\end{frame}


\section*{Thanks!}
\end{document}