\documentclass[xcolor=pdftex,dvipsnames,table,mathserif,aspectratio=169]{beamer}
\usetheme{metropolis}
%\usepackage{times}
%\usefonttheme{structurebold}

\usepackage[english]{babel}
%\usepackage[table]{xcolor}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps}
\usepackage{amsmath,amssymb,setspace,centernot}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{relsize}
\usepackage{pdfpages}
\usepackage[absolute,overlay]{textpos} 


\newenvironment{reference}[2]{% 
  \begin{textblock*}{\textwidth}(#1,#2) 
      \footnotesize\it\bgroup\color{red!50!black}}{\egroup\end{textblock*}} 

\DeclareMathSizes{10}{10}{6}{6} 

\begin{document}
\title{Part 6: Model Selection and Intro to ML}
\author{Chris Conlon}
\institute{Applied Econometrics II}
\date{\today}

\frame{\titlepage}


\begin{frame}
\frametitle{Reading}
\begin{itemize}
\item Chapters 2-7 of \textit{Elements of Statistical Learning}
\end{itemize}

\end{frame}



\section{Intro}
\frame{\frametitle{Overview}
How many components should we include in our model?
\begin{itemize}
\item Too few: under-fitting and large residuals.
\item Too many: over-fitting and poor out of sample prediction.
\end{itemize}
How do we choose?
\begin{itemize}
\item $X$ variables.
\item Instrumental Variables.
\end{itemize}
}


\begin{frame}
\frametitle{When do we have too much data?}
\begin{itemize}
\item On the internet!
\item Hedonics: What really determines the price of your house?
\item Prediction: What really determines loan defaults?
\item Consideration Sets: How many products do consumers really choose among on the shelf?
\item Which elements of financial filings really matter?
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{What do people mostly do in practice?}
\begin{itemize}
\item Regress $Y$ on $X$ with all variables included.
\item Drop some variables if they aren't significant?
\item Re-run with some things dropped
\item Add in some other things that may or may not be significant.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Nested and Non-nested Models}
What makes a model \alert{nested} or \alert{non-nested}?
\begin{align*}
y_i = \beta_0 + \beta_1 x_i + \beta_2 w_i + \beta_3 z_i + \varepsilon_i 
\end{align*}
A nested model can be written as a restricted version of the larger model
\begin{itemize}
\item ie: all of the following are nested within the model above
\begin{align*}
y_i &= \beta_0 + \beta_1 x_i + \beta_2 w_i + \varepsilon_i  \\
y_i &= \beta_0 + \beta_1 x_i + \beta_3 z_i + \varepsilon_i  \\
y_i &= \beta_0 + \beta_2 w_i + \beta_3 z_i + \varepsilon_i  
\end{align*}
\item ie: this model is non-nested (because of $s_i$)
\begin{align*}
y_i = \beta_0 + \beta_1 x_i + \beta_2 w_i + \beta_3 z_i + \gamma s_i +  \varepsilon_i 
\end{align*}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{What we teach undergrads}
\small
Two traditional ways to select the number of components in a model:\\
\begin{eqnarray*}
\overline{R}^2  &=& 1-SSR(p)/TSS - SSR(p)/TSS \cdot \frac{p}{N-p-1} \\
AIC(p) &=& \ln\left(\frac{SSR(p)}{N}\right) + (p+1)\frac{2}{N}\\
BIC(p) &=& \ln \left(\frac{SSR(p)}{N} \right) + (p+1)\frac{\ln N}{N}
\end{eqnarray*}
\begin{itemize}
\item Commonly employed in macroeconometric or time-series context for things like selecting lags of an autoregression
\begin{eqnarray*}
y_t = \alpha_0 + \sum_{k=1}^p \alpha_k y_{t-k} + \varepsilon_t
\end{eqnarray*}
\item These are designed for strictly \alert{nested} models.
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Review AIC/BIC}
\begin{itemize}
\item AIC tends to select larger models than BIC since it penalizes the number of parameters less heavily.
\item These usually depend on ordering potential models by $p$ the number of components and then sequentially fitting them.
\item AIC is not consistent: as $N \rightarrow \infty$ it may still select too many parameters.
\item BIC is consistent: as $N \rightarrow \infty$ it will select the correct number of parameters.
\item Of course for finite-sample $N < \infty$ anything can happen.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Where does it come from?}
How do we come up with these penalized regressions?
\begin{itemize}
\item AIC/BIC arise from considering the likelihood ratio test (LRT) of a maximum likelihood estimator and making a lot of assumptions.
\item AIC arises from minimizing the Expected KLIC.
\begin{eqnarray*}
KLIC(f,g) &=& \int  f(y) \log(f(y)) \partial y - \int  f(y) \log(g(y)) \partial y  \\
&=& C_f - E  \log(g(y))
\end{eqnarray*}
\item Low values of KLIC mean the models are similar.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Where does it come from?}
How do we come up with these penalized regressions?
\begin{itemize}
\item Recall that OLS is a ML estimator in the case where $\varepsilon$ is normally distributed.
\begin{eqnarray*}
D = - 2 \ln \left (\frac{\mbox{Likelihood  } H_0}{\mbox{ Likelihood  }H_a} \right) = -2 \ln \underbrace{\left(\frac{(\sup L(\theta | x) : \theta \in \Theta_0)}{(\sup L(\theta | x) : \theta \in \Theta)}\right)}_{\Lambda(x)}
\end{eqnarray*}
\item If the models are \alert{nested} then $\Theta_0 \subset \Theta$ and $\dim(\Theta) -\dim(\Theta_0) = q$ then as $N\rightarrow \infty$ we have that $D \rightarrow^d \chi^2(q)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Non-nested cases}
Many cases we are interested in are \alert{not strictly nested}
\begin{itemize}
\item Should I include $x_2$ OR $x_3$ in my regression? (partially overlapping)
\item Is the correct distribution $f(y | x, \theta)$ normal or log-normal? (non-overlapping)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Non-nested cases}
\begin{itemize}
\item Cox (1961) suggested the following (often infeasible solution) by assuming that $F_{\theta}$ is the true model.
\begin{eqnarray*}
LR(\hat{\theta},\hat{\gamma}) = L_f(\hat{\theta}) - L_g (\hat{\gamma}) = \sum_{i=1}^N \ln \frac{f(y_i | x_i, \hat{\theta})}{g(y_i | x_i, \hat{\gamma})}
\end{eqnarray*}
\item Depending on which the true model is you could reject $F_{\theta}$ for $G_{\gamma}$ and vice versa!
\item Deriving the test statistic is hard (and specific to $F_{\theta}$) because we must obtain $E_f [\ln \frac{f(y_i | x_i, \hat{\theta})}{g(y_i | x_i, \hat{\gamma})}]$.
\item Similar to AIC in that we are minimizing KLIC over $F_{\theta}$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Vuong Test}
\small
\begin{eqnarray*}
H_0: E_{h(y|x)} \left[ \frac{ f(y | x,\theta) }{g(y | x, \gamma)} \right] = 0  \\
\rightarrow E_h[\ln(h/g)] - E_h[\ln (h/f)] = 0
\end{eqnarray*}
\begin{itemize}
\item Instead of taking expectation with respect to one of two distributions, we take it with respect to $h(y |x)$ the unknown but \alert{true distribution}.
\item Same as testing whether two densities $(f,g)$ have same KLIC.
\item The main result is that (details in 8.5 of CT):
\begin{eqnarray*}
\frac{1}{\sqrt{N}} LR(\hat{\theta},\hat{\gamma}) \rightarrow^d N [0,\omega_{*}^2],\quad \omega_{*}^2 =  V_0 \left[ \ln \frac{f(y| x, \hat{\theta})}{g(y| x, \hat{\gamma})}  \right]
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Back to the real world...}
\begin{itemize}
\item We have some theoretical benchmark which lets us discern which of two model we prefer (under certain assumptions).
\item In practice we often start with a functional form like:\\
 $y_i = \beta_0 + \sum_{k=1}^p \beta_k x_{i,k} + \varepsilon_{i}$
 \item Which $x$'s do we include?
 \item Which $x$'s do we leave out?
 \item It is not clear that BIC/AIC or Vuong test tells us what we should do in practice.
 \item If you have $K$ potential regressors you could consider all $2^K$ possible regressions.
 \item Or you could could consider all $K \choose P$ possible combinations with $p$ parameters.
 \item This sounds very time consuming
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Things to keep in mind}
Two major (related) problems:
\begin{itemize}
\item Regressors are correlated with one another:
\begin{itemize}
\item small changes in the sample: $\beta_1$ goes up, $\beta_2$ goes down. 
\item large coefficients can lead to wild predictions.
\item If relationship between $y_i$ and $x_i$ nonlinear, and $x_i,z_i$ are highly correlated then we may attribute some of this nonlinearity to $z_i$, even when it has no effect.
\end{itemize}
\item Lots of imprecisely estimated parameters can make prediction tricky
\begin{itemize}
\item Small changes in the sample can lead to large changes in $\hat{y}_i | x_i$.
\end{itemize}
\end{itemize}
The big idea: maybe we tolerate some \alert{bias} to greatly reduce \alert{variance}.
\begin{itemize}
\item This is where ML and Econometrics diverge!
\item Everything you learned so far has been focused on \alert{unbiasedness}
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Minimizing SSR/AIC: all possible regressions}
\begin{center}
\includegraphics[height=\textheight]{./resources/subsetsaic}
\end{center}
\end{frame}

\begin{frame}
\frametitle{What is orthogonality?}
\begin{itemize}
\item We can think about a world where $\langle\, x_j, x_k \rangle =0$ for $j \neq k$.
\item In this world I can get $\beta_j$ by regressing $y$ on $x_j$ by simple linear regression.
\item I could do this for each $j$ and the resulting vector $\beta$ would be the same as running multiple regression.
\item We could try and transform $X$ so that it forms an \alert{orthogonal basis}.
\item Unless we are running regressions by hand this doesn't seem tremendously helpful.
\item However, in practice this is often what your software does!
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Gram-Schmidt/QR Decomposition}
\small
\begin{enumerate}
\item Let $x_0 = z_0 = 1$
\item For $j = 1,2,\ldots p$: Regress $x_j$ on $z_0,z_1,\ldots,z_{j-1}$  to give you $\hat{\gamma_{jl}} = \langle\, z_l, x_j \rangle/\langle\, z_l, x_l \rangle$ and residual $z_j = x_j  - \sum_{k=0}^{j-1} \hat{\gamma_{kj}} z_k$.
\item With your transformed orthogonal basis $\mathbf{z}$ you can now regress $y$ on $z_p$ one by one to obtain $\hat{\beta}_p$.
\end{enumerate}
What does this do?
\begin{itemize}
\item The resulting vector $\hat{\beta}$ has been adjusted to deliver the marginal contribution of $x_j$ on $y$ after adjusting for all $x_{-j}$.
\item If $x_j$ is highly correlated with other $x_k$'s then the residual $z_j$ will be close to zero and the coefficient will be unstable.
\item This will be true for any variables $x_l$ within a set of correlated variables.
\item We can delete any one of them to resolve this issue.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{QR Decomposition (Technical Details)}
\small
QR Decomposition has a matrix form which regression software uses:
\begin{eqnarray*}
\mathbf{X} &=& \mathbf{Z \Gamma} \\
    &=& \mathbf{\underbrace{Z D^{-1}}_{Q} \underbrace{D \Gamma}_{R}} \\
    \hat{\beta} &=& \mathbf{R^{-1} Q' y}\\
    \hat{\mathbf{y}} &=& \mathbf{Q Q'} \mathbf{y}
\end{eqnarray*}
\vspace{-0.5cm}
\begin{itemize}
\item $Z$ is the matrix of the orthogonalized residuals $z_j$'s.
\item $\Gamma$ is upper triangular matrix with entries $\hat{\gamma}_{kj}$
\item $D$ is diagonal matrix with entries $|| z_j ||$.
\item $Q$ is $N \times ((p+1)$ orthogonal  matrix $Q'Q = I$ 
\item $R$ is $(p+1) \times (p+1)$ upper triangular matrix.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{What happens in practice?}
What are people likely doing in practice:
\begin{itemize}
\item Start with a single $x$ variable and then slowly add more until additional $x$'s were insignificant
\item Start with all possible $x$ variables and drop those where $t$-statistics were insignificant.
\item These procedures actually make some sense if the columns of $X$ are \alert{linearly independent} or \alert{orthogonal}.
\item In practice our regressors are often correlated (sometimes highly so).
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Forward Stepwise Regression}
Consider the following \alert{greedy algorithm}
\begin{enumerate}
\item Start with an empty model and add a constant $\overline{y}$.
\item Then run $K$ single-variable regressions, choose the $x_k$ with the highest $t$-statistic call this $x^{(1)}$.
\item Now run $K-1$ two variable regressions where the constant and $x^{(1)}$ and choose $x^{(2)}$ as regression where $x_k$ has the highest t-statistic.
\item Now run $K-2$ three variable regressions where the constant and $x^{(1)},x^{(2)}$
\item You get the idea!
\end{enumerate}
We stop when the $x_k$ with the highest t-statistic is below some threshold (often 20\% significance).
\end{frame}


\begin{frame}
\frametitle{Backwards Stepwise Regression}
\begin{enumerate}
\item Start with an full model.
\item Remove the $x$ variable with the lowest $t$-statistic. Call this $x^{(k)}$.
\item Re-run the regression without $x^{(k)}$.
\item Repeat until the smallest $t$-statistic exceeds some threshold.
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Comparison}
\begin{itemize}
\item Backwards and fowards stepwise regression tend to give similar choices (but not always).
\item Everything is trivial if $X$'s columns are orthogonal (computer has some tricks otherwise- $QR$).
\item Forward stepwise works when we have more regressors than observations $K > N$.
\item I proposed the $t$-stat here but some packages use AIC/BIC as the criteria.
\item We should also be careful to \alert{group dummy variables together} as a single regressor.
\item These are implemented in \texttt{step} in R and \texttt{stepwise} in Stata.
\item We probably want to adjust our standard errors for the fact that we have run many regressions in sequence before arriving at our model. \alert{In practice not enough people do this!}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multiple Testing Problem}
\begin{itemize}
\item A big deal in Econometrics frequently ignored in applied work is the \alert{Multiple Testing Problem}
\item You didn't just pick the regression in your table and run that without considering any others.
\item This means that your $t$ and $F$ stats are going to be too large!! (Standard errors too small!)
\item How much bigger should they be?
\begin{itemize}
\item Analytic size corrections can be tricky and data dependent
\item Bootstrap/Monte-Carlo studies should give you a better idea.
\end{itemize}
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{(Incremental) Forward Stagewise Regression}
As an alternative consider:
\begin{enumerate}
\item Start with $r= y$ and $(\beta_1, \ldots, \beta_p) = 0$.
\item Find the predictor $x_j$ most correlated with $r$.
\item Update $\beta_j \leftarrow \beta_j + \delta_j$ where $\delta_j = \epsilon \cdot sgn \langle r , x_j \rangle$.
\item Update $r \leftarrow r - \delta_j \cdot x_j$ and repeat for $S$ steps.
\end{enumerate}
\begin{itemize}
\item Alternative $\delta_j =  \langle r , x_j \rangle$
\item We can continue until no regressors have correlation with residuals 
\item This is very slow (it takes many many $S$).
\item Sometimes slowness can be good -- in high dimensions to avoid overfitting.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Stepwise selection proedures}
\begin{center}
\includegraphics[height=0.9\textheight]{./resources/subsetstepwise}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Penalized Regression}
Suppose we fit a regression model and penalized extra variables all in one go, what would that look like?
\begin{eqnarray*}
\hat{\beta} = \arg \min_{\beta} \left[\frac{1}{2} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^p | \beta_j|^{q} \right]
\end{eqnarray*}
\begin{itemize}
\item We can consider the penalty term $\lambda \sum_{j=1}^p | \beta_j|^{q}$ as penalizing models where $\beta$ gets further away from zero.
\item Similar to placing a \alert{prior distribution} on $\beta_j$ centered at 0.
\item We definitely want to \alert{standardize} our inputs before using penalized regression methods.
\item Usually you fix $q$ and then look at how estimates respond to $\gamma$.
\item There are two famous cases $q=1$ (Lasso) and $q=2$ (Ridge) though in practice there are many possibilities.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{LASSO Regression}
\small
\begin{eqnarray*}
\hat{\beta}^{LASSO} = \arg \min_{\beta} \left[\frac{1}{2} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^K x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^K | \beta_j| \right]
\end{eqnarray*}
\begin{itemize}
\item Penalty is $L_1$ norm on $\beta$.
\item Can re-write as a constriant  $\sum_{j=1}^K | \beta_j| \leq s$
\item If $X$ is orthonormal then $\hat{\beta}_{j}^{LASSO} = sign(\hat{\beta}_j ) \cdot (| \hat{\beta}_j |- \lambda )_{+}$
\item In words: we get coefficients that are closer to zero by $\lambda$, but coefficients within $\lambda$ of zero are shrunk to zero.
\item This leads people to describe LASSO as a \alert{shrinkage} estimator. It produces models that are \alert{sparse}.
\item Instead of a discrete parameter such as the number of lags $p$ we can continuously penalize additional complexity with $\lambda$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{LASSO Regression}
But... is choosing $\lambda$ any easier than choosing $p$?
\begin{itemize}
\item We call $\lambda$ the \alert{regularization} parameter.
\item We can choose $\lambda$ in a way that minimizes expected prediction error (EPE).
\item Recall $EPE(\lambda) = E_x E_{y|x} ([ Y- g(X,\lambda)]^2 | X)$.
\item In practice most people look at out of sample prediction error rate on a \alert{cross validated sample}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LASSO Path}
\begin{center}
\includegraphics[height=0.9\textheight]{./resources/lassopath}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Ridge Regression}
Another popular alternative is the $q=2$ case
\begin{eqnarray*}
\hat{\beta}^{Ridge} = \arg \min_{\beta} \left[\frac{1}{2} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^K x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^K | \beta_j|^2 \right]
\end{eqnarray*}
\begin{itemize}
\item Penalty is $L_2$ norm on $\beta$.
\item Can re-write as a constriant  $\sum_{j=1}^K | \beta_j|^2 \leq s$
\item $\hat{\beta}^{Ridge} = (X'X + \lambda I )^{-1} X' Y$.
\item If $X$ is orthonormal then $\hat{\beta}_{j}^{Ridge} =  \hat{\beta}_j /(1 +\lambda )$
\item In words: everything gets dampened by a constant factor $\lambda$ (we don't get zeros).
\item Adding a constant to the diagonal of $(X'X)$ ensures that the matrix will be invertible even when we have multicollinearity.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ridge Path}
\begin{center}
\includegraphics[width=0.85\textheight]{./resources/ridgepath}
\end{center}
\end{frame}

\begin{frame}
\frametitle{LASSO vs Ridge}
\begin{center}
\includegraphics[width=\textheight]{./resources/geometry}
\end{center}
\end{frame}

\begin{frame}
\frametitle{What is the point?}
Ridge:
\begin{itemize}
\item Ridge doesn't provide sparsity which can be a good thing.
\item It is most helpful (relative to OLS) when $X$'s are highly correlated with one another.
\item OLS can set large but imprecise coefficients when it cannot disentangle effects.
\end{itemize}
LASSO:
\begin{itemize}
\item LASSO is useful for variable/feature selection.
\item LASSO does not generally posses the \alert{oracle property} though variants such as \alert{adaptive LASSO} may.
\item LASSO sometimes has the oracle property for $p \gg N$ and cases where the true $\beta$'s are not too large.
\item People sometimes use LASSO to choose components and then OLS for unbiased coefficient estimateds
\end{itemize}
\end{frame}

\begin{frame}{Elastic Net}
We can actually combine them using \alert{elastic net regression}:
\begin{eqnarray*}
 P(\lambda_1,\lambda_2,\mathbf{\beta}) =  \lambda _1\sum_{j=1}^K | \beta_j|  +\lambda_2 \sum_{j=1}^K | \beta_j|^2 
 \end{eqnarray*}
 This includes both the $L_1$ and $L_2$ penalties.
\end{frame}

\begin{frame}
\frametitle{LASSO vs. Ridge}
\begin{center}
\includegraphics[width=3.5in]{./resources/orthcompare}
\end{center}
\end{frame}


\begin{frame}
\frametitle{LAR: Least Angle Regression}
Remember Forward Stagewise Regression, consider this alternative:
\begin{enumerate}
\item Start with $r= y-\overline{y}$ and $(\beta_1, \ldots, \beta_p) = 0$. (Standardize first!)
\item Find the predictor $x_j$ most correlated with $r$.
\item Move $\beta_j$ from 0 to its least-squares estimate $\langle<x_j , r \rangle$ slowly
\item Update $r \leftarrow r - \delta_j \cdot x_j$.
\item Keep moving $x_j$ in same direction until $x_k$ has as much correlation with updated $r$,
\item Continue updating $(\beta_j, \beta_k)$ in direction of \alert{joint} least-squares coefficients until some other competitor $x_l$ has as much correlation with $r$.
\item Continue until all $p$ predictors have entered. After $\min[N-1,p]$ steps we arrive at full OLS solution.
\end{enumerate}
\begin{itemize}
\item \alert{Optional:} If a current least-squares estimate hits zero drop it from the active set and re-estimate the joint least squares direction without it.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{LAR: Least Angle Regression}
Why do we need LAR?
\begin{itemize}
\item It turns out that with the optional step from the previous slide: LAR gives us an easy algorithm to compute the LASSO estimate.
\item Actually it does even better -- it gives us the full path of LASSO estimates for all values of $\lambda$!
\item This is actually a relatively new result.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{LASSO vs LAR}
\begin{center}
\includegraphics[width=3.5in]{./resources/lar-lasso}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Overall Comparison}
\begin{center}
\includegraphics[width=3.5in]{./resources/comparisons}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Overall Comparison}
\begin{center}
\includegraphics[width=4in]{./resources/regressiontable}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Overall Comparison}
\begin{center}
\includegraphics[width=3.5in]{./resources/compareall}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Implementation}
\begin{itemize}
\item Routines are highly specialized: there are lots of tricks
\item No hope of coding this up on your own!
\item In R you can use \texttt{glmnet} or \texttt{lars}.
\item In Python you can use \texttt{scikit-learn}
\item In most recent Matlab in Stats toolbox you have \texttt{lasso} and \texttt{ridge}.
\item In STATA you can download \texttt{.ado} files from Christian Hansen's website (Chicago).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Other Data Reduction Techniques}
We have other data reduction techniques with a long history in Econometrics
\begin{itemize}
\item Principal Components
\item Factor Analysis
\item Partial Least Squares
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principal Components}
Suppose we have a very high dimensional $X$ where we have a high degree of correlation among the components $x_j$. 
\begin{itemize}
\item We can start by computing the appropriate correlation matrix $C=E[\tilde{X}'\tilde{X}]$ where $\tilde{X}$ denotes we have standardized each column $x_j$ to have mean zero and variance 1.
\item Diagonlize $C$ via the eigen-decomposition $V^{-1} C V = D$ where $D$ is the diagonal matrix of eigenvalues.
\item Sort $D$ and the corresponding columns of $V$ in decreasing order of the eigenvalues $d_j$.
\item Choose a subset of $m < K$ eigenvalues and eigenvectors and call that $V_m$ and $\lambda_m$
\item Compute transformed data: $Z_m = V_m \tilde{X}$ which is of dimension $N \times m$ instead of $N \times K$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principal Components}
\begin{itemize}
\item If $m \ll K$ then we can substantially reduce the dimension of the data.
\item The idea is to choose $m$ so that $(Z'Z)$ spans approximately the same space that $(X'X)$ does.
\item This works because we use the \alert{principal eigenvectors} (those with the largest eigenvalues).
\item The first eigenvector explains most of the variation in the data, the second the most of the remaining variation, and so on.
\item You may also recall that eigenvectors form an \alert{orthonormal basis}, so each dimension is linearly independent of the others.
\item As eigenvalues decline, it means they explain less of the variance.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principal Components}
Output from software will inlcude
\begin{itemize}
\item Coefficients: these transform from $X\rightarrow Z$
\item Score: these are the transformed $Z$'s
\item Latent/Eigenvalue: the corresponding Eigenvalue
\item Explained/Cumulative: cumulative explained variance $ \sum_{j=1}^m( \lambda_j / \sum_k \lambda_k)$
\item Stata: \texttt{pca}, Matlab: \texttt{pca}, R/stats: \texttt{princomp}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principal Components}
How many components?
\begin{itemize}
\item Choose \# of components by the eigenvalues or \% of variance explained
\item Common cutoffs are 90-95\% of variance.
\item Eigenvalue based cutoff rules (only take eigenvalues $ > 1$).
\item Most common method is to eyeball the scree plot.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Principal Components}
\begin{center}
\includegraphics[width=3.5in]{./resources/princomp1}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Principal Components}
\begin{center}
\includegraphics[height=0.9\textheight]{./resources/princomp2}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Principal Components}
\begin{itemize}
\item We can run a regression on principal components $Z$'s and then recover the betas of the $X$'s
\begin{eqnarray*}
\hat{y}_{(M)}^{pcr} = \overline{y} \mathbf{1} + \sum_{m=1}^M \hat{\theta}_m \mathbf{z}_m
\end{eqnarray*}
\item Because principal components are orthogonal we can find coefficients using univariate regression $\hat{\theta}_m = \langle \mathbf{z_m} , \mathbf{y} \rangle / \langle \mathbf{z_m} , \mathbf{z_m} \rangle$.
\item We can recover the $x$ coefficients because the PCA is a linear transformation:
\begin{eqnarray*}
\hat{\beta}^{PCR} = \sum_{m=1}^M \hat{\theta}_m v_m
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}{Principal Components (and Ridge)}
\begin{itemize}
\item If $M=P$ (we use all components) then PCR = OLS.
\item If $M < P$ then we discard the $p-M$ smallest eigenvalue components
\item This is similar to ridge which shrinks $\beta$'s for components with small eigenvalues.
\item Think about the variance matrix $X'X/n$ or $X' X = V D^2 V'$.
\item First component (largest eigenvalue) is $\mathbf{z_1} = \mathbf{X} v_1 = \mathbf{u_1} d_1$.
\item Variance is $Var(\mathbf{z_1}) = Var (\mathbf{X} v_1) = \frac{d_1^2}{N}$ ($\mathbf{z_1}$ is first principal component of $\mathbf{X}$).
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Principal Components}
\begin{center}
\includegraphics[width=4in]{./resources/princompfig}
\end{center}
\end{frame}




\begin{frame}
\frametitle{Principal Components And Ridge}
Consider the objective function that Ridge minimizes:
\begin{eqnarray*}
RSS(\lambda) = (\mathbf{y} - \mathbf{X} \beta)'  (\mathbf{y} - \mathbf{X} \beta)' + \lambda \beta' \beta
\end{eqnarray*}
And the solution (which addresses multicolinearity!)
\begin{eqnarray*}
\hat{\beta}_{ridge} = (\mathbf{X}' \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}' \mathbf{y}
\end{eqnarray*}

\end{frame}

\begin{frame}
\frametitle{Principal Components And Ridge}
Just like we can diagonalize (some) square matrices, we can take the \alert{singular value decomposition} (SVD) of any matrix $\mathbf{X}$ that is $N \times p$
\begin{eqnarray*}
\mathbf{X}=\mathbf{U} \mathbf{D} \mathbf{V}'
\end{eqnarray*}
\begin{itemize}
\item $\mathbf{U}, \mathbf{V}$ are $N \times p$ and $p \times p$ orthonormal matrices ($\mathbf{U}$ spans the column space, and $\mathbf{V}$ spans the row space of $X$.)
\item $\mathbf{D}$ is a diagonal matrix $p \times p$ with elements corresponding to the singular values of $\mathbf{X}$.
\item If $\mathbf{X}$ is a square, diagonalizable matrix the singular values are equal to the eigenvalues.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principal Components And Ridge}
Now the least squares solution is simple
\begin{eqnarray*}
\mathbf{X} \hat{\beta}^{ols} = \mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{y} = U U' \mathbf{y}.
\end{eqnarray*}
\begin{itemize}
\item $\mathbf{U}' \mathbf{y}$ are the values of $\mathbf{y}$ mapped into the orthonormal basis $\mathbf{U}$.
\item This looks a lot like $\mathbf{QR}$ except that we have chosen a different basis.
\end{itemize}
Ridge is simple too:
\begin{eqnarray*}
\mathbf{X} \hat{\beta}^{ridge} = \mathbf{X} (\mathbf{X}'\mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}' \mathbf{y} &=& \mathbf{U D} (\mathbf{D}^2 + \lambda \mathbf{I})^{-1} \mathbf{D} \mathbf{U}' \mathbf{y}\\
&=& \sum_{j=1}^p \mathbf{u}_j  \frac{d_j^2}{d_j^2 + \lambda} \mathbf{u}_j' \mathbf{y}
\end{eqnarray*}
\begin{itemize}
\item Same change of basis. Now we shrink each component by $d_j^2/(d_j^2 + \lambda)$.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Principal Components}
\begin{center}
\includegraphics[height=0.85\textheight]{./resources/pcrridge}
\end{center}
\end{frame}



\begin{frame}
\frametitle{Hansen Singleton (1982)}
\begin{itemize}
\item This is the original GMM example, though it comes from macro-finance not microeconometrics
\begin{eqnarray*}
\max_{c_{t+i},A_{t+i}}&& E_t \sum_{i=0}^{\infty} \frac{U(c_{t+i})}{(1+\delta)^i} \quad \mbox{subject to } \\
A_{t+i} &=& (1+r) A_{t+i-1} + y_{t+i} - c_{t+1} \\
0&=&\lim_{i \rightarrow \infty} E_t A_{t+i} (1+r)^{-i} 
\end{eqnarray*}
\item $A_t$ are your investment assets with return $r$ and discount factor $\delta$
\item $y_t$ is your income in period $t$, $c_t$ is your consumption 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hansen Singleton (1982)}
\begin{itemize}
\item Assume CRRA utility with risk aversion $\gamma$
\begin{eqnarray*}
U(c_{t+i})  = \frac{c_{t+i}^{1-\gamma}}{1-\gamma}
\end{eqnarray*}
\item We can take the first-order/Euler conditions and get:
\begin{eqnarray*}
E_t \left(\underbrace{\frac{1+r}{1+\delta} c_{t+1}^{-\gamma} - c_{t}^{-\gamma}}_{g(x_{t+i},\theta)}\right) = 0
\end{eqnarray*}
\item We want to estimate the ``deep parameters'' $\theta \equiv (\gamma,\delta)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hansen Singleton (1982)}
\begin{itemize}
\item We can solve for $\theta$ without actually solving the dynamic programming problem!
\item We just need some instruments $z_t$ that are conditionally independent/orthogonal to $g(x_{t+i},\theta)$ so that
\begin{eqnarray*}
E_t [g(x_{t+i},\theta) | z_t] = 0 \Rightarrow E_t [g(x_{t+i},\theta)  z_t] = 0 
\end{eqnarray*}
\item This is \alert{nonlinear GMM}. We need a matrix of instruments $z_t$ with dimension $N \times Q$ where $Q \geq \dim(\theta)$.
\item Where do we get $z_t$? $\rightarrow$ Economic Theory!
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Hansen Singleton (1982)}
Consider $E_t [g(x_{t+i},\theta) | z_t] = 0$.
\begin{itemize}
\item The error arises from the error in the Euler equation: deviations between observed behavior and expected behavior.
\item If the model is true this optimization error should be independent of anything known to the agent at the time the decision was made.
\item We often write: $E_t [g(x_{t+i},\theta) | z_t, \Omega_t] = 0$ where $\Omega_t$ is everything known by the agent up until time $t$ (including the full history).
\item If we have some potential instrument $z_t$ and use the full history then $z_{t-1},z_{t-2},\ldots$ are all valid instruments
\item If we use the conditional moment restriction $E_t [g(x_{t+i},\theta) | z_t, \Omega_t] = 0$ then any nonlinear function of $z_t$ is also an instrument
\begin{eqnarray*}
E_t [g(x_{t+i},\theta) f(z_{t,t-1,\ldots,0}) ]  = 0
\end{eqnarray*}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Hansen Singleton (1982)}
\begin{itemize}
\item We have literally infinite possibilities to construct instruments $z_t, z_t^2, z_t^3, z_t \cdot z_{t-1}, \ldots$
\item But our instruments could be \alert{weak} even though we have many of them.
\item And they might be highly correlated with each other.
\item Carrasco (2012) suggests \alert{regularization} on the instruments first.
\item One possibility for $f(z_{t},z_{t-1},z_{t-2},\ldots)$ is to take several higher order interactions and take the first $Q$ principal components.
\item Even though we might have 100 instruments, after running PCA we might find that 99\% of our variation is only in 6 components. In that case we should not try and identify more than 6 parameters!
\item Conlon (2014) suggests this as a test of non-identification in nonlinear BLP-type GMM problems.
\end{itemize}
\end{frame}

\begin{frame}
\small
\frametitle{Factor Models}
\begin{itemize}
\item Related to PCA is the \alert{factor model}
\item These are frequently used in finance for asset pricing. 
\begin{eqnarray*}
r_i = b_0 + b_1 f_1 + b_2 f_2 + \ldots b_p f_p + e_i
\end{eqnarray*}
\item Typically we choose factors so that $E[f_i] = 0$ and $E[f_i e_i] = 0$ and that $Cov(f_i,f_j) = 0$ for $i \neq j$.
\item That is we choose scaled factors to form an orthogonal basis (which makes pricing assets easier).
\item Instead of choosing $f$ to best explain $X'X$ we choose it to best explain $r$ by taking linear combinations of our $X$'s.
\item CAPM is a single factor model (where the factor is the \alert{market return}).
\item Fama-French have expanded to a 5 factor model (book-to-market, market-cap, profitability, momentum)
\item Ross's APT is another form of a factor model.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Factor Models: Other Examples}
\small
\begin{itemize}
\item \textit{Eigenfaces} reduces your face to the first few eigenvalues -- this is how face detection works!
\item In psychometrics they use data from multiple tests to measure different forms of intelligence (mathematical reasoning, verbal, logical, spatial, etc.)
\begin{itemize}
\item An old literature searched for general intelligence factor $g$
\item Nobody can tell what the GMAT measured!
\end{itemize}
\item In marketing PCA/factor analyses are used in the construction of \textit{perceptual maps}
\item Marketing practitioners use FA/PCA more than academics these days (guess: maybe?)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Oracle Property}
\begin{itemize}
\item An important question with LASSO is whether or not it produces consistent parameter estimates (Generally \alert{no}).
\item We think of asymptotics as taking both $N,p\rightarrow \infty$.
\item Donohu (2006) shows that for $p > N$ case, when the true model is sparse, LASSO identified correct predictors with high probability (with certain assumptions on $\mathbf{X}$) as we slowly relax the penalty.
\item Condition looks like (``good'' variables are not too correlated with ``bad'' variables).
\begin{eqnarray*}
\max_{j \in S^{c}} || x_j' X_{S} (X_{S}' X_{S})^{-1} ||_{1} \leq (1 -\epsilon) 
\end{eqnarray*}
\item Where $X_{S}$ are columns corresponding to nonzero coefficients, and $S^{c}$ are set of columns with zero coefficients (at true value).
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Other Extensions}
\begin{itemize}
\item \textit{Grouped LASSO} for penalzing groups of coefficients at once (like fixed effects)
\item \textit{Relaxed LASSO} run LASSO to select coefficients and then run a non-penalized subset regression or LASSO with a less stringent penalty on the subset. (Here CV tends to pick a less strong penalty term $\lambda$ leading to less shrinkage and bias).
\item \textit{SCAD: Smoothly Clipped Absolute Deviation}: do less shrinkage on big coefficients but preserve sparsity
\begin{eqnarray*}
\frac{ d J_a(|beta,\lambda)}{d \beta} = \lambda \cdot sgn(\beta) \left[ I(| \beta| \leq \lambda) + \frac{(a \lambda - | \beta|)_{+}}{(a-1) \lambda} I (| \beta| > \lambda) \right]
\end{eqnarray*}
\item \textit{Adaptive LASSO} uses  a weighted penalty of the form $\sum_{j=1}^p w_j |\beta_j|$ where $W_j = 1/|\hat{\beta}_j|^{\nu}$ using the OLS estimates as weights. This yields consistent estimates of parameters while retaining the convexity property.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Penalty Comparisons}
\begin{center}
\includegraphics[width=4.5in]{./resources/lassopenalty}
\end{center}
\end{frame}




\end{document}
